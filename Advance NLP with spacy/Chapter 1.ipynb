{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "chubby-obligation",
   "metadata": {},
   "source": [
    "# **Chapter 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-ottawa",
   "metadata": {},
   "source": [
    "# Finding words, phrases, names and concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "humanitarian-recall",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-purchase",
   "metadata": {},
   "source": [
    "## Doc, token, Span Lexical objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "naked-rachel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "World\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "# Doc objevt\n",
    "\n",
    "doc = nlp('Hello World!')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "finnish-eligibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World\n"
     ]
    }
   ],
   "source": [
    "# token\n",
    "\n",
    "token = doc[1]\n",
    "print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "electronic-arcade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World!\n"
     ]
    }
   ],
   "source": [
    "# Span object\n",
    "\n",
    "doc = nlp('Hello World!')\n",
    "\n",
    "span = doc[1:3]\n",
    "print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "opposed-granny",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'spacy.tokens.span.Span'>\n",
      "<class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "# Sentence is a doc\n",
    "# 2 or more words in a sentence is span (generally done by slicing [:])\n",
    "# Word in a sentence is token (generally done by [])\n",
    "\n",
    "doc = nlp('Manoj is a good boy')\n",
    "span = nlp('Manoj is a good boy')[0:2]\n",
    "token = nlp('Manoj is a good boy')[0]\n",
    "\n",
    "print(type(doc))\n",
    "print(type(span))\n",
    "print(type(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "descending-isolation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: [0, 1, 2, 3, 4]\n",
      "Text: ['It', 'costs', '$', '5', '.']\n",
      "is_alpha: [True, True, False, False, False]\n",
      "is_punct: [False, False, False, False, True]\n",
      "like_num: [False, False, False, True, False]\n"
     ]
    }
   ],
   "source": [
    "# Lexical Attributes\n",
    "\n",
    "doc = nlp('It costs $5.')\n",
    "\n",
    "print('Index:', [token.i for token in doc])\n",
    "print('Text:', [token.text for token in doc])\n",
    "\n",
    "print('is_alpha:', [token.is_alpha for token in doc])\n",
    "print('is_punct:', [token.is_punct for token in doc])\n",
    "print('like_num:', [token.like_num for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-mercy",
   "metadata": {},
   "source": [
    "## Model packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "corporate-briefing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON\n",
      "ate VERB\n",
      "the DET\n",
      "pizza NOUN\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp('She ate the pizza')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "impressed-identifier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON nsubj ate\n",
      "ate VERB ROOT ate\n",
      "the DET det pizza\n",
      "pizza NOUN dobj ate\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tight-hungarian",
   "metadata": {},
   "source": [
    "## Predicting named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "flush-humidity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "UK GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Apple is looking at buying UK startup for $1 billion')\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "norwegian-newspaper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries, cities, states\n",
      "noun, proper singular\n",
      "direct object\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain('GPE'))\n",
    "print(spacy.explain('NNP'))\n",
    "print(spacy.explain('dobj'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-charity",
   "metadata": {},
   "source": [
    "## Rule based matching (Matcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "peripheral-subject",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6139850947917451393, 1, 3)]\n",
      "iphone X\n"
     ]
    }
   ],
   "source": [
    "# eg 1:\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [{'TEXT' : 'iphone'}, {'TEXT': 'X'}]\n",
    "\n",
    "# matcher.add('IPhone_PATTER', None, pattern)\n",
    "\n",
    "matcher.add('IPhone', [pattern])\n",
    "\n",
    "doc = nlp('Upcoming iphone X release date leak')\n",
    "\n",
    "matches = matcher(doc)\n",
    "print(matches)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "piano-blind",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(7778788780550260102, 0, 5)]\n",
      "2018 FIFA world cup:\n"
     ]
    }
   ],
   "source": [
    "# eg 2:\n",
    "\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [{'IS_DIGIT' : True}, \n",
    "           {'LOWER': 'fifa'},\n",
    "           {'LOWER': 'world'},\n",
    "           {'LOWER': 'cup'},\n",
    "           {'IS_PUNCT' : True}\n",
    "          ]\n",
    "\n",
    "matcher.add('fifa', [pattern])\n",
    "\n",
    "doc = nlp('2018 FIFA world cup: France won!')\n",
    "\n",
    "matches = matcher(doc)\n",
    "print(matches)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "roman-barcelona",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3702023516439754181, 1, 3), (3702023516439754181, 6, 8)]\n",
      "loved cats\n",
      "love dogs\n"
     ]
    }
   ],
   "source": [
    "# eg 3:\n",
    "\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [{'LEMMA' : 'love', 'POS': 'VERB'},\n",
    "           {'POS': 'NOUN'}\n",
    "          ]\n",
    "\n",
    "matcher.add('love', [pattern])\n",
    "\n",
    "doc = nlp('I loved cats but now I love dogs more.')\n",
    "\n",
    "matches = matcher(doc)\n",
    "print(matches)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "documentary-regulation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8656102463236116519, 1, 3), (8656102463236116519, 8, 9), (8656102463236116519, 11, 14)]\n",
      "Solar Power\n",
      "solarpower\n",
      "Solar-power\n"
     ]
    }
   ],
   "source": [
    "# eg 4:\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern1 = [{'LOWER':'solarpower'}]\n",
    "pattern2 = [{'LOWER':'solar'},{'IS_PUNCT':True},{'LOWER':'power'}]\n",
    "pattern3 = [{'LOWER':'solar'},{'LOWER':'power'}]\n",
    "\n",
    "matcher.add('SolarPower', [pattern1,pattern2,pattern3])\n",
    "doc = nlp(\"The Solar Power industry continues to grow a solarpower increases. Solar-power is good\")\n",
    "found_matches = matcher(doc)\n",
    "print(found_matches)\n",
    "\n",
    "\n",
    "for _,start,end in found_matches:\n",
    "    span = doc[start:end]\n",
    "    print(span)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-progressive",
   "metadata": {},
   "source": [
    "## Using operators and quantifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "specific-spouse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3702023516439754181, 1, 4), (3702023516439754181, 8, 10)]\n",
      "bought a smartphone\n",
      "buying apps\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [{'LEMMA' : 'buy'}, \n",
    "           {'POS': 'DET', 'OP':'?'},\n",
    "           {'POS': 'NOUN'}\n",
    "          ]\n",
    "\n",
    "matcher.add('love', [pattern])\n",
    "\n",
    "doc = nlp(\"I bought a smartphone. Now I'm buying apps\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "print(matches)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-proposition",
   "metadata": {},
   "source": [
    "## Stop words in Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "minute-amazon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nor', 'anywhere', 'except', 'itself', 'among', 'all', 'him', 'really', 'they', 'beside', 'an', 'almost', 'both', 'during', 'we', 'towards', '‘re', 'another', '‘m', 'such', 'always', 'empty', 'because', 'nevertheless', 'he', 'become', 'was', 'latter', '’ve', 'ever', 'top', 'did', 'whereafter', '’d', 'to', 'she', \"'ve\", 'forty', 'last', 'what', 'as', 'me', 'indeed', 'so', 'or', 'else', 'into', '’s', 'could', 'once', 'many', 'ours', 'back', 'has', 'would', 'before', 'doing', \"'s\", 'part', 'keep', 'throughout', 'onto', 'wherever', 'us', 'above', 'but', 'thereafter', '‘d', 'amongst', 'in', 'go', 'never', 'still', 'am', 'cannot', 'does', 'on', 'when', 'around', 'sixty', 'behind', 'any', 'thereby', 'this', 'now', 'became', 'should', 'bottom', 'being', 'four', 'often', 'i', 'with', 'hereupon', 'wherein', 'move', 'why', 'less', 'regarding', 'there', 'ourselves', 'seeming', 'something', 'who', 'whole', 'three', 'her', 'whom', 'other', 'about', 'enough', 'hence', 'show', 'more', 'will', 'a', 'each', \"'ll\", 'third', 'yourself', 'seem', 'upon', 'nobody', \"'m\", 'its', 'only', 'hereafter', 'anything', 'yourselves', 'up', 'though', 'together', 'under', 'mine', 'somehow', 'through', 'against', 'due', 'is', 'no', 'nine', 'therefore', 'somewhere', 'toward', 'hers', 'seems', 'done', 'every', 'then', 'give', 'while', 'using', 'much', 'ten', 'get', 'themselves', 'whereby', 'namely', 'anyhow', 'hereby', 'nothing', 'ca', 'my', 'were', 'across', \"'re\", 'have', 'mostly', 'none', 'might', 'beforehand', 'moreover', 'someone', 'out', 'therein', 'without', 'whatever', 'whether', 'formerly', 'next', 'eleven', 'via', 'whoever', 'herein', 'it', 'everything', 'sometime', 'elsewhere', 'off', 'n‘t', 'the', 'do', 'quite', 'whereas', \"n't\", 'least', 'own', 'until', 'from', '’re', 'made', 'seemed', 'myself', 'put', 'just', 'neither', '‘ve', 'please', 'make', 'twenty', 'perhaps', 'former', 'these', 'may', 'at', 'himself', 'be', 'thereupon', 'others', 'few', 'latterly', 'two', 'anyway', 'been', 'some', 'although', 'here', 'by', 'already', 'say', 'take', 'fifty', 'even', 'meanwhile', 'becomes', 're', 'if', 'anyone', 'front', 'hundred', 'yours', 'not', 'his', 'otherwise', 'amount', 'see', 'our', 'how', 'for', 'which', 'whenever', 'their', 'along', 'serious', 'after', 'first', 'nowhere', 'same', 'used', \"'d\", 'call', 'rather', 'afterwards', 'five', 'your', 'must', 'again', 'whither', 'sometimes', 'those', 'thru', 'thence', 'well', 'and', 'noone', 'you', 'also', 'full', 'herself', 'however', 'fifteen', 'within', 'yet', '‘s', 'several', 'of', 'alone', 'between', 'over', 'further', 'n’t', 'everywhere', 'most', 'becoming', 'various', 'besides', 'too', 'thus', 'than', 'name', 'whence', 'where', 'down', 'very', 'since', '’m', 'side', 'can', 'them', 'twelve', '’ll', 'one', 'six', 'unless', 'whereupon', 'whose', 'are', '‘ll', 'per', 'eight', 'either', 'had', 'that', 'below', 'beyond', 'everyone'}\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-queue",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
