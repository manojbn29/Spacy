{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "derived-musician",
   "metadata": {},
   "source": [
    "# Finding words, phrases, names and concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "finished-elizabeth",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-tennessee",
   "metadata": {},
   "source": [
    "## Doc, token, Span Lexical objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "rolled-gallery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "World\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "# Doc objevt\n",
    "\n",
    "doc = nlp('Hello World!')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "desperate-being",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World\n"
     ]
    }
   ],
   "source": [
    "# token\n",
    "\n",
    "token = doc[1]\n",
    "print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "competitive-morris",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World!\n"
     ]
    }
   ],
   "source": [
    "# Span object\n",
    "\n",
    "doc = nlp('Hello World!')\n",
    "\n",
    "span = doc[1:3]\n",
    "print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "durable-palmer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: [0, 1, 2, 3, 4]\n",
      "Text: ['It', 'costs', '$', '5', '.']\n",
      "is_alpha: [True, True, False, False, False]\n",
      "is_punct: [False, False, False, False, True]\n",
      "like_num: [False, False, False, True, False]\n"
     ]
    }
   ],
   "source": [
    "# Lexical Attributes\n",
    "\n",
    "doc = nlp('It costs $5.')\n",
    "\n",
    "print('Index:', [token.i for token in doc])\n",
    "print('Text:', [token.text for token in doc])\n",
    "\n",
    "print('is_alpha:', [token.is_alpha for token in doc])\n",
    "print('is_punct:', [token.is_punct for token in doc])\n",
    "print('like_num:', [token.like_num for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-appeal",
   "metadata": {},
   "source": [
    "## Model packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "applied-canal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON\n",
      "ate VERB\n",
      "the DET\n",
      "pizza NOUN\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp('She ate the pizza')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "filled-arbitration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON nsubj ate\n",
      "ate VERB ROOT ate\n",
      "the DET det pizza\n",
      "pizza NOUN dobj ate\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-escape",
   "metadata": {},
   "source": [
    "## Predicting named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "indian-subscriber",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "UK GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Apple is looking at buying UK startup for $1 billion')\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "stuffed-margin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries, cities, states\n",
      "noun, proper singular\n",
      "direct object\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain('GPE'))\n",
    "print(spacy.explain('NNP'))\n",
    "print(spacy.explain('dobj'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-mathematics",
   "metadata": {},
   "source": [
    "## Rule based matching (Matcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "vulnerable-biography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iphone X\n"
     ]
    }
   ],
   "source": [
    "# eg 1:\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [{'TEXT' : 'iphone'}, {'TEXT': 'X'}]\n",
    "\n",
    "# matcher.add('IPhone_PATTER', None, pattern)\n",
    "\n",
    "matcher.add('IPhone', [pattern])\n",
    "\n",
    "doc = nlp('Upcoming iphone X release date leak')\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "floating-source",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(7778788780550260102, 0, 5)]\n",
      "2018 FIFA world cup:\n"
     ]
    }
   ],
   "source": [
    "# eg 2:\n",
    "\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [{'IS_DIGIT' : True}, \n",
    "           {'LOWER': 'fifa'},\n",
    "           {'LOWER': 'world'},\n",
    "           {'LOWER': 'cup'},\n",
    "           {'IS_PUNCT' : True}\n",
    "          ]\n",
    "\n",
    "matcher.add('fifa', [pattern])\n",
    "\n",
    "doc = nlp('2018 FIFA world cup: France won!')\n",
    "\n",
    "matches = matcher(doc)\n",
    "print(matches)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "material-greek",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3702023516439754181, 1, 3), (3702023516439754181, 6, 8)]\n",
      "loved cats\n",
      "love dogs\n"
     ]
    }
   ],
   "source": [
    "# eg 3:\n",
    "\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [{'LEMMA' : 'love', 'POS': 'VERB'},\n",
    "           {'POS': 'NOUN'}\n",
    "          ]\n",
    "\n",
    "matcher.add('love', [pattern])\n",
    "\n",
    "doc = nlp('I loved cats but now I love dogs more.')\n",
    "\n",
    "matches = matcher(doc)\n",
    "print(matches)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "noted-smith",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8656102463236116519, 1, 3), (8656102463236116519, 8, 9), (8656102463236116519, 11, 14)]\n",
      "Solar Power\n",
      "solarpower\n",
      "Solar-power\n"
     ]
    }
   ],
   "source": [
    "# eg 4:\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern1 = [{'LOWER':'solarpower'}]\n",
    "pattern2 = [{'LOWER':'solar'},{'IS_PUNCT':True},{'LOWER':'power'}]\n",
    "pattern3 = [{'LOWER':'solar'},{'LOWER':'power'}]\n",
    "\n",
    "matcher.add('SolarPower', [pattern1,pattern2,pattern3])\n",
    "doc = nlp(\"The Solar Power industry continues to grow a solarpower increases. Solar-power is good\")\n",
    "found_matches = matcher(doc)\n",
    "print(found_matches)\n",
    "\n",
    "\n",
    "for _,start,end in found_matches:\n",
    "    span = doc[start:end]\n",
    "    print(span)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-token",
   "metadata": {},
   "source": [
    "## Using operators and quantifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "sporting-spotlight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3702023516439754181, 1, 4), (3702023516439754181, 8, 10)]\n",
      "bought a smartphone\n",
      "buying apps\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [{'LEMMA' : 'buy'}, \n",
    "           {'POS': 'DET', 'OP':'?'},\n",
    "           {'POS': 'NOUN'}\n",
    "          ]\n",
    "\n",
    "matcher.add('love', [pattern])\n",
    "\n",
    "doc = nlp(\"I bought a smartphone. Now I'm buying apps\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "print(matches)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-spider",
   "metadata": {},
   "source": [
    "## Stop words in Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "valued-filename",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'therein', 'much', 'bottom', 'been', 'everyone', 'although', 'eight', 'yet', 'than', 'herein', '‘d', 'we', '’re', 'the', 'whom', 'elsewhere', 'both', 'mostly', 'top', 'down', 'four', 'too', 'two', 'somewhere', 'take', 'one', 'whereas', 'well', 'fifty', '‘m', 'where', 'so', 'may', 'front', 'nevertheless', 'hereafter', 'however', 'such', 'through', 'then', 'using', 'several', 'next', \"'m\", 'otherwise', 'becoming', 'him', 'thereafter', 'out', 'call', 'against', 'still', 'was', 'anyway', 'either', 'own', 'us', 'always', 'less', 'for', 'ten', 'afterwards', 'his', 'twenty', 'perhaps', 'each', 'became', \"'d\", 'noone', 'just', 'hers', 'is', '‘s', 'n‘t', 'now', 'around', 'nothing', 'ever', 'unless', 'whatever', 'she', 'yourself', 'besides', 'together', 'never', 'there', '‘re', 'often', 'keep', 'fifteen', 'former', 'same', 'even', 'about', 'already', 'beforehand', 'very', 'after', 'its', 'whereupon', 'neither', \"n't\", 'nor', 'show', 'hereupon', 'along', 'to', 'throughout', 'sometime', 'more', 'indeed', 'ca', 'put', 'whoever', 'moreover', 'go', 'thereby', 'various', 'does', 'themselves', 'under', '’ll', 'can', 'formerly', 'these', 'yours', 'amongst', 'them', 'because', 're', '‘ll', 'most', 'something', 'yourselves', 'except', 'beside', 'were', 'if', 'move', 'why', 'regarding', 'really', 'everywhere', 'give', 'ourselves', 'at', 'among', 'whence', 'anyone', 'they', 'seems', 'when', 'do', 'six', 'in', 'every', 'few', 'wherever', 'serious', 'had', 'off', 'some', 'you', 'become', 'seeming', 'forty', 'becomes', 'further', 'have', 'doing', 'upon', 'nobody', 'once', 'your', 'ours', '’d', 'namely', 'rather', 'toward', 'above', 'he', 'who', 'and', 'thru', 'least', 'whereby', '’ve', 'full', 'must', 'whether', 'it', 'whole', 'be', 'whither', 'here', 'almost', 'i', 'could', 'that', 'nine', 'an', 'our', 'seemed', 'nowhere', 'per', 'see', '‘ve', 'all', 'not', 'thence', 'by', 'being', 'five', 'anyhow', 'me', 'itself', 'sixty', 'am', 'across', 'as', 'used', \"'ll\", 'myself', 'cannot', 'others', 'onto', 'until', 'third', 'what', 'last', 'done', 'say', 'twelve', 'wherein', 'did', 'due', 'over', 'within', 'from', 'sometimes', 'meanwhile', 'seem', 'made', 'therefore', 'are', 'their', 'only', 'without', 'before', 'while', 'everything', 'empty', 'between', 'her', '’s', 'no', 'since', 'herself', 'thus', 'whenever', \"'s\", 'please', 'might', 'my', 'hereby', 'three', \"'ve\", 'again', '’m', 'how', 'also', 'which', 'thereupon', 'will', 'enough', 'else', 'has', 'quite', 'or', 'somehow', 'eleven', 'this', 'towards', 'first', 'other', 'part', 'many', 'make', 'latterly', 'get', 'name', 'himself', 'would', 'any', 'another', 'into', 'amount', 'side', 'alone', 'on', 'anything', 'below', 'should', 'beyond', 'none', 'hundred', 'via', \"'re\", 'anywhere', 'of', 'but', 'whose', 'mine', 'n’t', 'hence', 'whereafter', 'someone', 'with', 'up', 'a', 'during', 'latter', 'behind', 'though', 'those', 'back'}\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-dominican",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
